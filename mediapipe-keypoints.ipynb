{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-05-12T17:21:51.029192Z","iopub.execute_input":"2023-05-12T17:21:51.030342Z","iopub.status.idle":"2023-05-12T17:21:51.036273Z","shell.execute_reply.started":"2023-05-12T17:21:51.030290Z","shell.execute_reply":"2023-05-12T17:21:51.035287Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import shutil\n\ndef delete_extra_folders():\n    for root, dirname, filenames in os.walk('/kaggle/input'):\n        for dir in dirname:\n            if (dir=='extra' or dir=='Extra'):\n                file_path = os.path.join(root,dir)\n                if os.path.exists(file_path):\n                    print(file_path)\n                    shutil.rmtree(file_path)  \n#delete_extra_folders()","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:52:13.309127Z","iopub.execute_input":"2023-05-12T17:52:13.310099Z","iopub.status.idle":"2023-05-12T17:52:13.318711Z","shell.execute_reply.started":"2023-05-12T17:52:13.310047Z","shell.execute_reply":"2023-05-12T17:52:13.317501Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"pip install mediapipe","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:23:26.093185Z","iopub.execute_input":"2023-05-12T17:23:26.093649Z","iopub.status.idle":"2023-05-12T17:23:44.422857Z","shell.execute_reply.started":"2023-05-12T17:23:26.093606Z","shell.execute_reply":"2023-05-12T17:23:44.421237Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting mediapipe\n  Downloading mediapipe-0.9.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.0/33.0 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.21.6)\nRequirement already satisfied: matplotlib in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.5.3)\nRequirement already satisfied: protobuf<4,>=3.11 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (3.20.3)\nRequirement already satisfied: flatbuffers>=2.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (23.1.21)\nRequirement already satisfied: opencv-contrib-python in /opt/conda/lib/python3.7/site-packages (from mediapipe) (4.5.4.60)\nRequirement already satisfied: attrs>=19.1.0 in /opt/conda/lib/python3.7/site-packages (from mediapipe) (22.2.0)\nRequirement already satisfied: absl-py in /opt/conda/lib/python3.7/site-packages (from mediapipe) (1.4.0)\nRequirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (2.8.2)\nRequirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (4.38.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (1.4.4)\nRequirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (0.11.0)\nRequirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (3.0.9)\nRequirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (9.4.0)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from matplotlib->mediapipe) (23.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.4.0)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\nInstalling collected packages: mediapipe\nSuccessfully installed mediapipe-0.9.0.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nimport json\nimport multiprocessing\nimport argparse\nimport os.path\nimport cv2\nimport mediapipe as mp\nfrom tqdm.auto import tqdm\nfrom joblib import Parallel, delayed\nimport numpy as np\nimport gc\nimport warnings\n\ndef process_landmarks(landmarks):\n    x_list, y_list = [], []\n    if landmarks is not None:\n        for landmark in landmarks.landmark:\n            x_list.append(landmark.x)\n            y_list.append(landmark.y)\n    return x_list, y_list\n\n\ndef process_hand_keypoints(results):\n    hand1_x, hand1_y, hand2_x, hand2_y = [], [], [], []\n\n    if results.multi_hand_landmarks is not None:\n        if len(results.multi_hand_landmarks) > 0:\n            hand1 = results.multi_hand_landmarks[0]\n            hand1_x, hand1_y = process_landmarks(hand1)\n\n        if len(results.multi_hand_landmarks) > 1:\n            hand2 = results.multi_hand_landmarks[1]\n            hand2_x, hand2_y = process_landmarks(hand2)\n\n    return hand1_x, hand1_y, hand2_x, hand2_y\n\n\ndef process_pose_keypoints(results):\n    pose = results.pose_landmarks\n    pose_x, pose_y = process_landmarks(pose)\n    return pose_x, pose_y\n\n\ndef swap_hands(pose_x, pose_y, hand, input_hand):\n    left_wrist_x, left_wrist_y = np.nan, np.nan\n    right_wrist_x, right_wrist_y = np.nan, np.nan\n\n    if len(pose_x) >= 17 and len(pose_y) >= 17:\n        left_wrist_x, left_wrist_y = pose_x[15], pose_y[15]\n        right_wrist_x, right_wrist_y = pose_x[16], pose_y[16]\n        \n    hand_x, hand_y = hand\n\n    left_dist = (left_wrist_x - hand_x) ** 2 + (left_wrist_y - hand_y) ** 2\n    right_dist = (right_wrist_x - hand_x) ** 2 + (right_wrist_y - hand_y) ** 2\n\n    if left_dist < right_dist and input_hand == \"h2\":\n        return True\n\n    if right_dist < left_dist and input_hand == \"h1\":\n        return True\n\n    return False\n\n\ndef process_video(path, save_dir):\n    hands = mp.solutions.hands.Hands(\n        min_detection_confidence=0.5, min_tracking_confidence=0.5\n    )\n    pose = mp.solutions.pose.Pose(\n        min_detection_confidence=0.5, min_tracking_confidence=0.5#, upper_body_only=True\n    )\n\n    pose_points_x, pose_points_y = [], []\n    hand1_points_x, hand1_points_y = [], []\n    hand2_points_x, hand2_points_y = [], []\n    \n    \n    # skip processing if 'extra' or 'Extra' is present in path\n#     if 'extra' in path or 'Extra' in path:\n#         print(f\"Skipping {path}\")\n#         return\n    label = path.split(\"/\")[-2]\n    label = \"\".join([i for i in label if i.isalpha()]).lower()\n    uid = os.path.splitext(os.path.basename(path))[0]\n    uid = \"_\".join([label, uid])\n    n_frames = 0\n    if not os.path.isfile(path):\n        warnings.warn(path + \" file not found\")\n    cap = cv2.VideoCapture(path)\n    while cap.isOpened():\n        ret, image = cap.read()\n        if not ret:\n            break\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        hand_results = hands.process(image)\n        pose_results = pose.process(image)\n\n        hand1_x, hand1_y, hand2_x, hand2_y = process_hand_keypoints(hand_results)\n        pose_x, pose_y = process_pose_keypoints(pose_results)\n        \n        ## Assign hands to correct positions\n        if len(hand1_x) > 0 and len(hand2_x) == 0:\n            if swap_hands(\n                 pose_x,\n                 pose_y,\n                 hand=(hand1_x[0], hand1_y[0]),\n                 input_hand=\"h1\",\n                ):\n                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n\n        elif len(hand1_x) == 0 and len(hand2_x) > 0:\n            if swap_hands(\n                 pose_x,\n                 pose_y,\n                 hand=(hand2_x[0], hand2_y[0]),\n                 input_hand=\"h2\",\n             ):\n                hand1_x, hand1_y, hand2_x, hand2_y = hand2_x, hand2_y, hand1_x, hand1_y\n\n         ## Set to nan so that values can be interpolated in dataloader\n        pose_x = pose_x if pose_x else [np.nan] * 25\n        pose_y = pose_y if pose_y else [np.nan] * 25\n\n        hand1_x = hand1_x if hand1_x else [np.nan] * 21\n        hand1_y = hand1_y if hand1_y else [np.nan] * 21\n        hand2_x = hand2_x if hand2_x else [np.nan] * 21\n        hand2_y = hand2_y if hand2_y else [np.nan] * 21\n\n        pose_points_x.append(pose_x)\n        pose_points_y.append(pose_y)\n        hand1_points_x.append(hand1_x)\n        hand1_points_y.append(hand1_y)\n        hand2_points_x.append(hand2_x)\n        hand2_points_y.append(hand2_y)\n\n        n_frames += 1\n\n    cap.release()\n\n    ## Set to nan so that values can be interpolated in dataloader\n    pose_points_x = pose_points_x if pose_points_x else [[np.nan] * 25]\n    pose_points_y = pose_points_y if pose_points_y else [[np.nan] * 25]\n\n    hand1_points_x = hand1_points_x if hand1_points_x else [[np.nan] * 21]\n    hand1_points_y = hand1_points_y if hand1_points_y else [[np.nan] * 21]\n    hand2_points_x = hand2_points_x if hand2_points_x else [[np.nan] * 21]\n    hand2_points_y = hand2_points_y if hand2_points_y else [[np.nan] * 21]\n\n    save_data = {\n        \"uid\": uid,\n        \"label\": label,\n        \"pose_x\": pose_points_x,\n        \"pose_y\": pose_points_y,\n        \"hand1_x\": hand1_points_x,\n        \"hand1_y\": hand1_points_y,\n        \"hand2_x\": hand2_points_x,\n        \"hand2_y\": hand2_points_y,\n        \"n_frames\": n_frames,\n    }\n    with open(os.path.join(save_dir, f\"{uid}.json\"), \"w\") as f:\n        json.dump(save_data, f)\n\n    hands.close()\n    pose.close()\n    del hands, pose, save_data\n    gc.collect()\n\ndef correct_paths(paths):\n    \"\"\"\n    If the video is not in include 50m then it can be in include-50-2, so update path to check there.\n    And also check if MOV or MP4 exension works\n    \"\"\"\n    n = len(paths)\n    for i in range(n):\n        if not os.path.exists(paths[i]):\n            new_path = paths[i].replace('MOV','MP4')\n            if(os.path.exists(new_path)):\n                paths[i] = new_path\n            else:\n                paths[i] = paths[i].replace('include-50','include-50-2')\n                if(not os.path.exists(paths[i])):\n                    paths[i] = paths[i].replace('MOV','MP4')\n               \n    return paths\n\ndef load_file(path, include_dir):\n    with open(path, \"r\") as fp:\n        data = fp.read()\n        data = data.split(\"\\n\")\n    data = list(map(lambda x: os.path.join(include_dir, x), data))\n    data = correct_paths(data)\n    return data\n\n\ndef load_train_test_val_paths(args):\n    train_paths = load_file(\n        f\"/kaggle/input/train-test-paths/train_test_paths/{args['dataset']}_train.txt\", args['include_dir']\n    )\n    val_paths = load_file(f\"/kaggle/input/train-test-paths/train_test_paths/{args['dataset']}_val.txt\", args['include_dir'])\n    test_paths = load_file(\n        f\"/kaggle/input/train-test-paths/train_test_paths/{args['dataset']}_test.txt\", args['include_dir']\n    )\n    return train_paths, val_paths, test_paths\n\ndef save_keypoints(dataset, file_paths, mode, args):\n    save_dir = os.path.join(args['save_direc'], f\"{dataset}_keypoints\")\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n    \n    Parallel(n_jobs=n_cores, backend=\"multiprocessing\")(\n        delayed(process_video)(path, save_dir)\n        for path in tqdm(file_paths, desc=f\"processing videos\")\n    )\n#689 train\n#77 val\n#192 test\n\nargs = {\n    'include_dir':'/kaggle/input/include-50/',\n    #/kaggle/input/include-50-2/\n    'save_direc':'/kaggle/working/keypoints1/',\n    'dataset':'include50'\n}\n\nn_cores = multiprocessing.cpu_count()\ntrain_paths, val_paths, test_paths = load_train_test_val_paths(args)\n\n\n# args['save_direc']='/kaggle/working/val_keypoints/'\n# save_keypoints('include50', val_paths, \"val\", args)\nargs['save_direc']='/kaggle/working/test_keypoints/'\nsave_keypoints('include50', test_paths, \"test\", args)\n\nargs['save_direc']='/kaggle/working/train_keypoints/'\nsave_keypoints('include50', train_paths, \"train\", args)","metadata":{"execution":{"iopub.status.busy":"2023-05-12T17:53:18.923159Z","iopub.execute_input":"2023-05-12T17:53:18.923606Z","iopub.status.idle":"2023-05-12T18:40:52.595237Z","shell.execute_reply.started":"2023-05-12T17:53:18.923562Z","shell.execute_reply":"2023-05-12T18:40:52.593515Z"},"trusted":true},"execution_count":26,"outputs":[{"output_type":"display_data","data":{"text/plain":"processing videos:   0%|          | 0/192 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27396e1a1df1476da1330a4032aa05c1"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3742.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3740.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4920.MP4 file not found\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5207.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4041.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5186.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4898.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_5211.MP4 file not found\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"processing videos:   0%|          | 0/689 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9af2a833c6d548e483f1c300cd2a82b7"}},"metadata":{}},{"name":"stderr","text":"INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\nINFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4018.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_5061.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4899.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4923.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5185.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_3743.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5038.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_3867.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_5062.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4207.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5058.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_3721.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_3745.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4922.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_3891.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_8612.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4043.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5206.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_3892.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3889.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5039.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3887.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_3720.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4921.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_3719.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4919.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4019.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4044.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5184.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_5209.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_8620.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_3890.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4040.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4918.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4216.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5059.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3888.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_3868.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_4215.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_4042.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_5037.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_8619.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5208.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_3741.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/55. White/MVI_5210.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/54. Black/MVI_5060.MP4 file not found\n/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:89: UserWarning: /kaggle/input/include-50-2/Colours/47. Red/MVI_4017.MP4 file not found\n","output_type":"stream"}]},{"cell_type":"code","source":"#Execute only this box vee\nimport pandas as pd\nimport json\nimport glob\n\ndef convert_json_to_dataframe(file_paths):\n    data = []\n    for file_path in file_paths:\n        with open(file_path, 'r') as file:\n            json_data = json.load(file)\n            data.append(json_data)\n    \n    df = pd.DataFrame(data, columns=[\"uid\", \"label\", \"pose_x\", \"pose_y\", \"hand1_x\", \"hand1_y\", \"hand2_x\", \"hand2_y\", \"n_frames\"])\n    return df\n\n# Example usage\nval_file_paths = glob.glob(\"/kaggle/working/val_keypoints/include50_keypoints/*.json\")\ntest_file_paths = glob.glob(\"/kaggle/working/test_keypoints/include50_keypoints/*.json\")\ntrain_file_paths = glob.glob(\"/kaggle/working/train_keypoints/include50_keypoints/*.json\")\n\nval_df = convert_json_to_dataframe(val_file_paths)\ntest_df = convert_json_to_dataframe(test_file_paths)\ntrain_df = convert_json_to_dataframe(train_file_paths)\n\n#convert to csvs\nval_df.to_csv('val.csv')\ntest_df.to_csv('test.csv')\ntrain_df.to_csv('train.csv')","metadata":{"execution":{"iopub.status.busy":"2023-05-12T18:41:53.610835Z","iopub.execute_input":"2023-05-12T18:41:53.612101Z","iopub.status.idle":"2023-05-12T18:42:12.898134Z","shell.execute_reply.started":"2023-05-12T18:41:53.612007Z","shell.execute_reply":"2023-05-12T18:42:12.896497Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Vee, u can ignore below this","metadata":{}},{"cell_type":"code","source":"args = {'data_dir':'ls',\n       'dataset':'include50',\n       'use_augs': False,\n       'save_dir': '/kaggle/working/'}","metadata":{"execution":{"iopub.status.busy":"2023-05-12T08:04:40.860874Z","iopub.execute_input":"2023-05-12T08:04:40.861534Z","iopub.status.idle":"2023-05-12T08:04:40.869379Z","shell.execute_reply.started":"2023-05-12T08:04:40.861477Z","shell.execute_reply":"2023-05-12T08:04:40.867667Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\n\nclass Augmentation:\n    def __init__(self, aug_func, p=1):\n        self.aug_func = aug_func\n        self.p = p\n\n    def __call__(self, df):\n        if np.random.rand() <= self.p:\n            return self.aug_func(df)\n        return df\n\n\ndef OneOf(aug_a, aug_b):\n    if np.random.rand() < 0.5:\n        return aug_a\n    return aug_b\n\n\ndef plus7rotation(df):\n    # +7 degree rotation\n    df_augmented = pd.DataFrame()\n    df_augmented[\"uid\"] = df[\"uid\"]\n    df_augmented[\"pose\"] = \"\"\n    df_augmented[\"hand1\"] = \"\"\n    df_augmented[\"hand2\"] = \"\"\n    df_augmented[\"label\"] = df[\"label\"]\n\n    theta = 7 * (np.pi / 180)\n    c, s = np.cos(theta), np.sin(theta)\n    rotation_matrix = np.array([[c, -s], [s, c]])\n\n    for i in range(df.shape[0]):\n        for col in [\"pose\", \"hand1\", \"hand2\"]:\n            matrix = np.array(df.loc[i, col], dtype=np.float)\n            matrix = np.matmul(matrix, rotation_matrix)\n            matrix = np.where(np.isnan(matrix), None, matrix).tolist()\n            df_augmented.at[i, col] = matrix\n\n    return df_augmented\n\n\ndef minus7rotation(df):\n    # -7 degree rotation\n    df_augmented = pd.DataFrame()\n    df_augmented[\"uid\"] = df[\"uid\"]\n    df_augmented[\"pose\"] = \"\"\n    df_augmented[\"hand1\"] = \"\"\n    df_augmented[\"hand2\"] = \"\"\n    df_augmented[\"label\"] = df[\"label\"]\n\n    theta = -7 * (np.pi / 180)\n    c, s = np.cos(theta), np.sin(theta)\n    rotation_matrix = np.array([[c, -s], [s, c]])\n\n    for i in range(df.shape[0]):\n        for col in [\"pose\", \"hand1\", \"hand2\"]:\n            matrix = np.array(df.loc[i, col], dtype=np.float)\n            matrix = np.matmul(matrix, rotation_matrix)\n            matrix = np.where(np.isnan(matrix), None, matrix).tolist()\n            df_augmented.at[i, col] = matrix\n\n    return df_augmented\n\n\ndef gaussSample(df):\n    # Random Gaussian sampling\n    df_augmented = df.copy()\n    dv = 0.05 * 10 ** -2\n    sv = 0.08 * 10 ** -2\n    lv = 0.08 * 10 ** -1\n    sigma = [\n        sv,\n        dv,\n        dv,\n        dv,\n        dv,\n        dv,\n        dv,\n        sv,\n        sv,\n        sv,\n        sv,\n        lv,\n        lv,\n        lv,\n        lv,\n        sv,\n        sv,\n        sv,\n        sv,\n        sv,\n        sv,\n        sv,\n        sv,\n        lv,\n        lv,\n    ]\n\n    ## Check if keypoints is range [0, 1]\n    x_width = 1920\n    y_height = 1080\n    for i in range(df.shape[0]):\n        if np.count_nonzero(df.loc[i, \"pose\"]) == 0:\n            break\n\n        pose = np.array(df.loc[i, \"pose\"], dtype=np.float)\n        pose[:, 0] /= x_width\n        pose[:, 1] /= y_height\n        pose_variance = np.column_stack((sigma, sigma))\n        pose = np.random.normal(pose, pose_variance)\n        pose[:, 0] *= x_width\n        pose[:, 1] *= y_height\n        pose = np.where(np.isnan(pose), None, pose).tolist()\n\n        hand1 = np.array(df.loc[i, \"hand1\"], dtype=np.float)\n        hand1[:, 0] /= x_width\n        hand1[:, 1] /= y_height\n        hand1 = np.random.normal(hand1, dv)\n        hand1[:, 0] *= x_width\n        hand1[:, 1] *= y_height\n        hand1 = np.where(np.isnan(hand1), None, hand1).tolist()\n\n        hand2 = np.array(df.loc[i, \"hand2\"], dtype=np.float)\n        hand2[:, 0] /= x_width\n        hand2[:, 1] /= y_height\n        hand2 = np.random.normal(hand2, dv)\n        hand2[:, 0] *= x_width\n        hand2[:, 1] *= y_height\n        hand2 = np.where(np.isnan(hand2), None, hand2).tolist()\n\n        df_augmented.at[i, \"pose\"] = pose\n        df_augmented.at[i, \"hand1\"] = hand1\n        df_augmented.at[i, \"hand2\"] = hand2\n\n    return df_augmented\n\n\ndef cutout(df):\n    # cutout\n    df_augmented = df.copy()\n\n    pad_idx = 0\n    for i in range(df.shape[0]):\n        if np.count_nonzero(df.loc[i, \"pose\"]) == 0:\n            pad_idx = i\n            break\n\n    for i in range(df.shape[0]):\n        if np.count_nonzero(df.loc[i, \"pose\"]) == 0:\n            break\n\n        if i < pad_idx:\n            pose = np.array(df.loc[i, \"pose\"])\n            hand1 = np.array(df.loc[i, \"hand1\"])\n            hand2 = np.array(df.loc[i, \"hand2\"])\n            pose_zero_idx = np.random.choice(25, 3, replace=False)\n            hand1_zero_idx = np.random.choice(21, 3, replace=False)\n            hand2_zero_idx = np.random.choice(21, 3, replace=False)\n\n            for i in pose_zero_idx:\n                pose[i] = [0, 0]\n            for i in hand1_zero_idx:\n                hand1[i] = [0, 0]\n            for i in hand2_zero_idx:\n                hand2[i] = [0, 0]\n\n            pose = pose.tolist()\n            hand1 = hand1.tolist()\n            hand2 = hand2.tolist()\n\n            df_augmented.at[i, \"pose\"] = pose\n            df_augmented.at[i, \"hand1\"] = hand1\n            df_augmented.at[i, \"hand2\"] = hand2\n\n    return df_augmented\n\n\ndef downsample(df):\n    # downsample\n    frame_len = df.shape[0]\n    if frame_len < 15:\n        return df.copy()\n\n    df_augmented = df.copy()\n    drop_idx = np.random.choice(frame_len, 15)  # 154 frames , 15 frames\n    df_augmented = df_augmented.drop(index=drop_idx)\n    return df_augmented\n\n\ndef upsample(df):\n    # upsample\n    def get_avg(df, idx, col):\n        aug_points = (\n            (\n                np.array(df.loc[idx - 1, col], dtype=np.float)\n                + np.array(df.loc[idx, col], dtype=np.float)\n            )\n            / 2\n        ).tolist()\n        return np.where(np.isnan(aug_points), None, aug_points).tolist()\n\n    frame_length = df.shape[0]\n    additional_frames = frame_length // 10\n    df_augmented = pd.DataFrame(\n        index=np.arange(frame_length + additional_frames),\n        columns=[\"uid\", \"pose\", \"hand1\", \"hand2\", \"label\"],\n    )\n    df_augmented[\"uid\"] = df.iloc[0].loc[\"uid\"]\n\n    j = 0\n    for i in range(df_augmented.shape[0]):\n        if i % 10 != 0 or i == 0:\n            df_augmented.at[i, \"pose\"] = df.loc[j, \"pose\"]\n            df_augmented.at[i, \"hand1\"] = df.loc[j, \"hand1\"]\n            df_augmented.at[i, \"hand2\"] = df.loc[j, \"hand2\"]\n            j += 1\n            continue\n\n        df_augmented.at[i, \"pose\"] = get_avg(df, j, \"pose\")\n        df_augmented.at[i, \"hand1\"] = get_avg(df, j, \"hand1\")\n        df_augmented.at[i, \"hand2\"] = get_avg(df, j, \"hand2\")\n\n    df_augmented[\"label\"] = df.iloc[0].loc[\"label\"]\n    return df_augmented","metadata":{"execution":{"iopub.status.busy":"2023-05-12T07:56:42.576410Z","iopub.execute_input":"2023-05-12T07:56:42.576928Z","iopub.status.idle":"2023-05-12T07:56:42.636743Z","shell.execute_reply.started":"2023-05-12T07:56:42.576877Z","shell.execute_reply":"2023-05-12T07:56:42.634641Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"import os\n\nfrom sklearn.metrics import accuracy_score\nimport pandas as pd\nimport numpy as np\n\n#from models import Xgboost\n#from configs import XgbConfig\n#from utils import get_experiment_name, load_label_map\n# from augment import (\n#     plus7rotation,\n#     minus7rotation,\n#     gaussSample,\n#     cutout,\n#     upsample,\n#     downsample,\n# )\nfrom tqdm.auto import tqdm\n\ndef load_label_map(dataset):\n    file_path = f\"label_maps/label_map_{dataset}.json\"\n    return load_json(file_path)\n\n\ndef get_experiment_name(args):\n    exp_name = \"\"\n    if args.use_cnn:\n        exp_name += \"cnn_\"\n    if args['use_augs']:\n        exp_name += \"augs_\"\n    exp_name += args.model\n    return exp_name\n\ndef flatten(arr, max_seq_len=200):\n    arr = np.array(arr)\n    arr = np.pad(arr, ((0, max_seq_len - arr.shape[0]), (0, 0)), \"constant\")\n    arr = arr.flatten()\n    return arr\n\n\ndef combine_xy(x, y):\n    x, y = np.array(x), np.array(y)\n    _, length = x.shape\n    x = x.reshape((-1, length, 1))\n    y = y.reshape((-1, length, 1))\n    return np.concatenate((x, y), -1).astype(np.float32)\n\n\ndef split_xy(data):\n    value_x, value_y = [], []\n    for row in data:\n        row = np.asarray(row)\n        if row.shape == ():\n            continue\n        value_x.append(row[:, 0])\n        value_y.append(row[:, 1])\n    value_x, value_y = np.asarray(value_x), np.asarray(value_y)\n    return value_x, value_y\n\n\ndef augment_sample(df, augs):\n    df = df.copy()\n    pose = combine_xy(df.pose_x, df.pose_y)\n    h1 = combine_xy(df.hand1_x, df.hand1_y)\n    h2 = combine_xy(df.hand2_x, df.hand2_y)\n    input_df = pd.DataFrame.from_dict(\n        {\n            \"uid\": df.uid,\n            \"pose\": pose.tolist(),\n            \"hand1\": h1.tolist(),\n            \"hand2\": h2.tolist(),\n            \"label\": df.label,\n        }\n    )\n    augmented_samples = []\n    for augmentation in augs:\n        df_augmented = augmentation(input_df)\n        pose_x, pose_y = split_xy(df_augmented.pose)\n        hand1_x, hand1_y = split_xy(df_augmented.hand1)\n        hand2_x, hand2_y = split_xy(df_augmented.hand2)\n        save_df = pd.Series(\n            {\n                \"uid\": df.uid + \"_\" + augmentation.__name__,\n                \"label\": df.label,\n                \"pose_x\": pose_x.tolist(),\n                \"pose_y\": pose_y.tolist(),\n                \"hand1_x\": hand1_x.tolist(),\n                \"hand1_y\": hand1_y.tolist(),\n                \"hand2_x\": hand2_x.tolist(),\n                \"hand2_y\": hand2_y.tolist(),\n                \"n_frames\": df.n_frames,\n            }\n        )\n        augmented_samples.append(save_df)\n\n    return pd.concat(augmented_samples, axis=0)\n\n\ndef preprocess(df, use_augs, label_map, mode):\n    feature_cols = [\"pose_x\", \"pose_y\", \"hand1_x\", \"hand1_y\", \"hand2_x\", \"hand2_y\"]\n    x, y = [], []\n    i = 0\n    no_of_videos = df.shape[0]\n    pbar = tqdm(total=no_of_videos, desc=f\"Processing {mode} file....\")\n    while i < no_of_videos:\n        if use_augs and mode == \"train\":\n            augs = [\n                plus7rotation,\n                minus7rotation,\n                gaussSample,\n                cutout,\n                upsample,\n                downsample,\n            ]\n            augmented_rows = augment_sample(df.iloc[i], augs)\n            df = pd.concat([df, augmented_rows], axis=0)\n        row = df.loc[i, feature_cols]\n        flatten_features = np.hstack(list(map(flatten, row.values)))\n        x.append(flatten_features)\n        y.append(label_map[df.loc[i, \"label\"]])\n        i += 1\n        pbar.update(1)\n    x = np.stack(x)\n    y = np.array(y)\n    return x, y\n\ndef load_dataframe(files):\n    series = []\n    for file_path in files:\n        series.append(pd.read_json(file_path, typ=\"series\"))\n    return pd.concat(series, axis=0)\n\n\ndef fit(args):\n    train_files = sorted(\n        glob.glob(\"/kaggle/working/train_keypoints/include50_keypoints/*.json\")\n    \n    val_files = sorted(\n        glob.glob(\"/kaggle/working/val_keypoints/include50_keypoints/*.json\")\n        )\n    \n    train_df = load_dataframe(train_files)\n    val_df = load_dataframe(val_files)\n\n    label_map = load_label_map(args['dataset'])\n    x_train, y_train = preprocess(train_df, args['use_augs'], label_map, \"train\")\n    x_val, y_val = preprocess(val_df, args['use_augs'], label_map, \"val\")\n\n    config = XgbConfig()\n    model = Xgboost(config=config)\n    model.fit(x_train, y_train, x_val, y_val)\n\n    exp_name = get_experiment_name(args)\n    save_path = os.path.join(args.save_dir, exp_name, \".pickle.dat\")\n    model.save(save_path)\n\n\ndef evaluate(args):\n    test_files = sorted(\n        glob.glob(\n            os.path.join(args.data_dir, f\"{args.dataset}_test_keypoints\", \"*.json\")\n        )\n    )\n\n    test_df = load_dataframe(test_files)\n\n    label_map = load_label_map(args.dataset)\n    x_test, y_test = preprocess(test_df, args.use_augs, label_map, \"test\")\n\n    exp_name = get_experiment_name(args)\n    config = XgbConfig()\n    model = Xgboost(config=config)\n    load_path = os.path.join(args.save_dir, exp_name, \".pickle.dat\")\n    model.load(load_path)\n    print(\"### Model loaded ###\")\n\n    test_preds = model(x_test)\n    print(\"Test accuracy:\", accuracy_score(y_test, test_preds))\n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-12T08:14:30.387840Z","iopub.execute_input":"2023-05-12T08:14:30.388355Z","iopub.status.idle":"2023-05-12T08:14:30.479495Z","shell.execute_reply.started":"2023-05-12T08:14:30.388315Z","shell.execute_reply":"2023-05-12T08:14:30.477117Z"},"trusted":true},"execution_count":78,"outputs":[{"name":"stdout","text":"Train Files:\n[]\nVal Files:\n[]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/936846300.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;31m#     print(\"Test accuracy:\", accuracy_score(y_test, test_preds))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/tmp/ipykernel_27/936846300.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m     \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    152\u001b[0m     \u001b[0mval_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_27/936846300.py\u001b[0m in \u001b[0;36mload_dataframe\u001b[0;34m(files)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0mseries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"series\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    302\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m     )\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 351\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No objects to concatenate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkeys\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: No objects to concatenate"],"ename":"ValueError","evalue":"No objects to concatenate","output_type":"error"}]},{"cell_type":"markdown","source":"Ignore following code. Pursuin different approach","metadata":{}},{"cell_type":"markdown","source":"To DO for group:\n1. make a dataframe, like previous one. hand1x,hand1y, hand2x, hand2y, label, n_frames.\n2. instead of processing all frames, only select mid 30? or 40? and then process them.\n3. write a function to extract keypoints, for that we have to understand the structure of hand_landmarker_result","metadata":{}},{"cell_type":"code","source":"#our box\n\nimport mediapipe as mp\n\nBaseOptions = mp.tasks.BaseOptions\nHandLandmarker = mp.tasks.vision.HandLandmarker\nHandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\nVisionRunningMode = mp.tasks.vision.RunningMode\n\n# Create a hand landmarker instance with the video mode:\noptions = HandLandmarkerOptions(\n    base_options=BaseOptions(model_asset_path='/kaggle/input/hand-landmark/hand_landmarker.task'),\n    running_mode=VisionRunningMode.VIDEO)\n\nwith HandLandmarker.create_from_options(options) as landmarker:\n    \n  # The landmarker is initialized. Use it here.\n  # ...\n    # Use OpenCV’s VideoCapture to load the input video.\n    cap = cv2.VideoCapture('/kaggle/input/include-50/Adjectives/1. loud/MVI_5177.MOV')\n    video_framerate = cap.get(cv2.CAP_PROP_FPS)\n    vdo_keypoints = []\n    \n    while cap.isOpened():\n        ret, image = cap.read() #read returns if frame_exits, current_frame\n        if not ret:\n            break\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) #when we load, its in blue green red, we convert it to RGB\n    # Load the frame rate of the video using OpenCV’s CV_CAP_PROP_FPS\n    # You’ll need it to calculate the timestamp for each frame.\n    # Loop through each frame in the video using VideoCapture#read()\n    # Convert the frame received from OpenCV to a MediaPipe’s Image object.\n        mp_image = mp.Image(image_format=mp.ImageFormat.SRGB, data=image)\n        # Perform hand landmarks detection on the provided single image.\n        # The hand landmarker must be created with the video mode.\n        frame_timestamp_ms = int(cap.get(cv2.CAP_PROP_POS_MSEC))\n        print(type(frame_timestamp_ms))\n        #case CV_FFMPEG_CAP_PROP_POS_MSEC:\n        #return 1000.0*(double)frame_number/get_fps();\n        hand_landmarker_result = landmarker.detect_for_video(mp_image, frame_timestamp_ms)\n        print(hand_landmarker_result)\n        hand1_x,hand1_y, hand2_x, hand2_y = process_hand_landmarker_result(hand_landmarker_result) \n        \n        \n        #process the output.\n        #refer last section of this link\n        #https://developers.google.com/mediapipe/solutions/vision/hand_landmarker/python#video_2\n        \n        \n        cap.release()\n        \n    \n    ","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.969227Z","iopub.status.idle":"2023-05-12T04:39:55.969644Z","shell.execute_reply.started":"2023-05-12T04:39:55.969432Z","shell.execute_reply":"2023-05-12T04:39:55.969456Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(hand_landmarker_result.hand_landmarks))","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.971153Z","iopub.status.idle":"2023-05-12T04:39:55.973128Z","shell.execute_reply.started":"2023-05-12T04:39:55.972777Z","shell.execute_reply":"2023-05-12T04:39:55.972812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(hand_landmarker_result.handedness))","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.974886Z","iopub.status.idle":"2023-05-12T04:39:55.975607Z","shell.execute_reply.started":"2023-05-12T04:39:55.975293Z","shell.execute_reply":"2023-05-12T04:39:55.975328Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(type(hand_landmarker_result.hand_world_landmarks))","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.977670Z","iopub.status.idle":"2023-05-12T04:39:55.978714Z","shell.execute_reply.started":"2023-05-12T04:39:55.978473Z","shell.execute_reply":"2023-05-12T04:39:55.978500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(hand_landmarker_result.hand_world_landmarks[0]))","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.980164Z","iopub.status.idle":"2023-05-12T04:39:55.981387Z","shell.execute_reply.started":"2023-05-12T04:39:55.981145Z","shell.execute_reply":"2023-05-12T04:39:55.981172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(hand_landmarker_result.hand_landmarks[0]))","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.982885Z","iopub.status.idle":"2023-05-12T04:39:55.983388Z","shell.execute_reply.started":"2023-05-12T04:39:55.983158Z","shell.execute_reply":"2023-05-12T04:39:55.983184Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hand_landmarker_result.hand_landmarks[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.984890Z","iopub.status.idle":"2023-05-12T04:39:55.985326Z","shell.execute_reply.started":"2023-05-12T04:39:55.985108Z","shell.execute_reply":"2023-05-12T04:39:55.985131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hand_landmarker_result.hand_world_landmarks[0]","metadata":{"execution":{"iopub.status.busy":"2023-05-12T04:39:55.987583Z","iopub.status.idle":"2023-05-12T04:39:55.988112Z","shell.execute_reply.started":"2023-05-12T04:39:55.987878Z","shell.execute_reply":"2023-05-12T04:39:55.987903Z"},"trusted":true},"execution_count":null,"outputs":[]}]}