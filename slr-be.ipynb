{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport cv2\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('/kaggle/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-03-11T04:37:45.830955Z","iopub.execute_input":"2023-03-11T04:37:45.831402Z","iopub.status.idle":"2023-03-11T04:37:46.048330Z","shell.execute_reply.started":"2023-03-11T04:37:45.831325Z","shell.execute_reply":"2023-03-11T04:37:46.047189Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ignore\n# GFG :Read the video from specified path\n#https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html?highlight=videocapture%20set#bool%20VideoCapture::set%28int%20propId,%20double%20value%29\n#cam = cv2.VideoCapture(\"kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV\")\n#cam.isOpened()\n#Doesn't work, retval : false","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Use following command b4 importing napari:\n#pip install napari[all] napari_video\n#if needed:\n#pip install xgboost \n#Resources- links used\n#https://github.com/janclemenslab/napari-video\n#https://forum.image.sc/t/working-with-mov-and-other-movie-files-in-python-dask/34053","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import napari\nfrom napari_video.napari_video import VideoReaderNP\n# /kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV\n#vr = VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV')\n#print(vr)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:12.369812Z","iopub.execute_input":"2023-03-11T04:38:12.370225Z","iopub.status.idle":"2023-03-11T04:38:12.394647Z","shell.execute_reply.started":"2023-03-11T04:38:12.370174Z","shell.execute_reply":"2023-03-11T04:38:12.393686Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"TRY THIS: First, we load the movie to be annotated. Since behavior movies can be quite long, we will use a lazy loading strategy (i.e., we will only load the frames as they are used). Using dask-image, we can construct an object that we can pass to napari for lazy loading in just one line. For more explanation on using dask to lazily load images in napari, see this tutorial.","metadata":{}},{"cell_type":"code","source":"vdos = []\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5136.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5297.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/89. warm/MVI_9495.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/89. warm/MVI_9336.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/90. cool/MVI_9337.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/90. cool/MVI_9338.MOV'))\n#vdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/91. new/MVI_9261.MOV'))\n#vdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/91. new/MVI_9421.MOV'))\n#vdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_5140.MOV'))\n\n#7-10 index test\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5295.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/90. cool/MVI_9339.MOV'))\nvdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/89. warm/MVI_5302.MOV'))\n#vdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/91. new/MVI_5148.MOV'))\n#vdos.append(VideoReaderNP('/kaggle/input/include/Adjectives_6of8/Adjectives/88. cold/MVI_9490.MOV'))\n#/kaggle/input/include/Clothes_2of2/Clothes/46. Clothing/MVI_5181.MOV","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:14.009743Z","iopub.execute_input":"2023-03-11T04:38:14.010189Z","iopub.status.idle":"2023-03-11T04:38:14.661069Z","shell.execute_reply.started":"2023-03-11T04:38:14.010143Z","shell.execute_reply":"2023-03-11T04:38:14.660067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train = pd.DataFrame(columns = [\"Class\"])\nY_test = pd.DataFrame([\"hot\",\"cool\",\"warm\"],columns = [\"Class\"])\nlabels = [\"hot\",\"hot\",\"warm\",\"warm\",\"cool\",\"cool\"]\nY_train[\"Class\"] = labels\nY_train, Y_test","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:30.433000Z","iopub.execute_input":"2023-03-11T04:38:30.433370Z","iopub.status.idle":"2023-03-11T04:38:30.464809Z","shell.execute_reply.started":"2023-03-11T04:38:30.433339Z","shell.execute_reply":"2023-03-11T04:38:30.463774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\nle_y = LabelEncoder()\nle_y.fit(Y_train)\nY_train = le_y.transform(Y_train[\"Class\"])\nY_test = le_y.transform(Y_test[\"Class\"])\nY_train","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:34.596554Z","iopub.execute_input":"2023-03-11T04:38:34.597163Z","iopub.status.idle":"2023-03-11T04:38:35.047494Z","shell.execute_reply.started":"2023-03-11T04:38:34.597127Z","shell.execute_reply":"2023-03-11T04:38:35.046566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_test","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:39.013448Z","iopub.execute_input":"2023-03-11T04:38:39.013829Z","iopub.status.idle":"2023-03-11T04:38:39.020207Z","shell.execute_reply.started":"2023-03-11T04:38:39.013793Z","shell.execute_reply":"2023-03-11T04:38:39.019117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ignore this nash, this is for openPose\n\n!git clone https://github.com/Hzzone/pytorch-openpose\n%cd pytorch-openpose\n\n#https://www.kaggle.com/code/rkuo2000/openpose-pytorch","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:42.280145Z","iopub.execute_input":"2023-03-11T04:38:42.280511Z","iopub.status.idle":"2023-03-11T04:38:45.086940Z","shell.execute_reply.started":"2023-03-11T04:38:42.280480Z","shell.execute_reply":"2023-03-11T04:38:45.085583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#this box is for openpose, ignore this nash\nimport matplotlib.pyplot as plt\nimport copy\nimport numpy as np\nfrom src import model\nfrom src import util\n#https://github.com/Hzzone/pytorch-openpose/blob/master/src/util.py\nfrom src.body import Body\nfrom src.hand import Hand","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:39:16.622525Z","iopub.execute_input":"2023-03-11T04:39:16.622910Z","iopub.status.idle":"2023-03-11T04:39:18.545247Z","shell.execute_reply.started":"2023-03-11T04:39:16.622879Z","shell.execute_reply":"2023-03-11T04:39:18.544267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!cp /kaggle/input/openpose-pretrained-models/body_pose_model.pth model\n!cp /kaggle/input/openpose-pretrained-models/hand_pose_model.pth model\n#it's asked to store pretrained models in folder named models","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:38:49.150186Z","iopub.execute_input":"2023-03-11T04:38:49.150564Z","iopub.status.idle":"2023-03-11T04:38:56.485431Z","shell.execute_reply.started":"2023-03-11T04:38:49.150529Z","shell.execute_reply":"2023-03-11T04:38:56.483874Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"body_estimation = Body('model/body_pose_model.pth')\nhand_estimation = Hand('model/hand_pose_model.pth')\n#.pth files are ML models created by PyTorch machine learning framework for Python.","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:39:22.481204Z","iopub.execute_input":"2023-03-11T04:39:22.481948Z","iopub.status.idle":"2023-03-11T04:39:26.767685Z","shell.execute_reply.started":"2023-03-11T04:39:22.481910Z","shell.execute_reply":"2023-03-11T04:39:26.766667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_all_hand_peaks(hands_list,oriImg):\n    all_hand_peaks = []\n    for x, y, w, is_left in hands_list:\n        peaks = hand_estimation(oriImg[y:y+w, x:x+w, :]) #pretrained model, last layer: convolution_param-> num_output: 22, 21 Keypoints\n        #peaks is coordinates wrt x,y top left.\n        #adjust coordinates of keypoints\n        peaks[:, 0] = np.where(peaks[:, 0]==0, peaks[:, 0], peaks[:, 0]+x)\n        peaks[:, 1] = np.where(peaks[:, 1]==0, peaks[:, 1], peaks[:, 1]+y) \n        all_hand_peaks.append(peaks)\n    return all_hand_peaks\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_hand_keypoints_for_one_video(video):\n    vdo_canvas = []\n    Num_of_frames = len(video)\n    for f in range(Num_of_frames):\n        oriImg = video[f]\n\n        candidate, subset = body_estimation(oriImg)\n        canvas = copy.deepcopy(oriImg)\n        hands_list = util.handDetect(candidate, subset, oriImg)\n        all_hand_peaks = get_all_hand_peaks(hands_list,oriImg)\n        canvas = util.draw_handpose(canvas, all_hand_peaks)\n        vdo_canvas.append(canvas)\n    return vdo_canvas","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#test_vdo = '/kaggle/input/include/Adjectives_6of8/Adjectives/87. hot/MVI_5138.MOV' This is labelled in hot but it is cold.\ntest_image = vdos[0][27]\n#Extract a frame\nimport matplotlib.pyplot as plt\n# image = plt.imread(test_image) \n#imread will take filename as a string\nplt.figure(figsize=(10,10))\nplt.imshow(test_image)\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:39:29.468618Z","iopub.execute_input":"2023-03-11T04:39:29.469011Z","iopub.status.idle":"2023-03-11T04:39:30.317468Z","shell.execute_reply.started":"2023-03-11T04:39:29.468981Z","shell.execute_reply":"2023-03-11T04:39:30.316647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Detect Body\n\nkeypoints index : index = int(subset[n][i])\n\nkeypoints coord : x,y = candidate[index][0:2]\nCandidates each element is a list of 4 elemnts. what are last 2?  first two are x,y co ordinates","metadata":{}},{"cell_type":"code","source":"oriImg = vdos[0][27]\n\ncandidate, subset = body_estimation(oriImg) #retval: 2 ndarrays\n#canvas is ndarray\ncanvas = copy.deepcopy(oriImg) #recursively copies, so that nested objs are also copied truly and any chnage on copy doesn't affect the original\ncanvas = util.draw_bodypose(canvas, candidate, subset)\nprint(len(candidate)) # number of keypoints\nprint(len(subset))    # number of persons","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:39:37.468427Z","iopub.execute_input":"2023-03-11T04:39:37.468824Z","iopub.status.idle":"2023-03-11T04:39:48.899316Z","shell.execute_reply.started":"2023-03-11T04:39:37.468788Z","shell.execute_reply":"2023-03-11T04:39:48.898102Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/src/openpose/hand/handDetector.cpp\n\nhands_list = util.handDetect(candidate, subset, oriImg)\n''' handDetect:\n    return value: [[x, y, w, True if left hand else False]].\n    width=height since the network require squared input.\n    x, y is the coordinate of top left \n'''\nall_hand_peaks = get_all_hand_peaks(hands_list,oriImg)\ncanvas = util.draw_handpose(canvas, all_hand_peaks)","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:40:12.184761Z","iopub.execute_input":"2023-03-11T04:40:12.185169Z","iopub.status.idle":"2023-03-11T04:40:13.278324Z","shell.execute_reply.started":"2023-03-11T04:40:12.185135Z","shell.execute_reply":"2023-03-11T04:40:13.276468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(canvas) #canvas[:, :, [2, 1, 0]]\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:40:35.620422Z","iopub.execute_input":"2023-03-11T04:40:35.620812Z","iopub.status.idle":"2023-03-11T04:40:35.848383Z","shell.execute_reply.started":"2023-03-11T04:40:35.620777Z","shell.execute_reply":"2023-03-11T04:40:35.847513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#https://github.com/CMU-Perceptual-Computing-Lab/openpose/blob/master/src/openpose/hand/handDetector.cpp\noriImg = vdos[4][23]\n\ncandidate, subset = body_estimation(oriImg) #retval: 2 ndarrays\n#canvas is ndarray\ncanvas = copy.deepcopy(oriImg) #recursively copies, so that nested objs are also copied truly and any chnage on copy doesn't affect the original\ncanvas = util.draw_bodypose(canvas, candidate, subset)\nprint(len(candidate)) # number of keypoints\nprint(len(subset))    # number of persons\n\nhands_list = util.handDetect(candidate, subset, oriImg)\n''' handDetect:\n    return value: [[x, y, w, True if left hand else False]].\n    width=height since the network require squared input.\n    x, y is the coordinate of top left \n'''\nall_hand_peaks = get_all_hand_peaks(hands_list,oriImg)\n\ncanvas = util.draw_handpose(canvas, all_hand_peaks)\nplt.figure(figsize=(10,10))\nplt.imshow(canvas) #canvas[:, :, [2, 1, 0]]\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:38:33.542027Z","iopub.execute_input":"2023-01-22T12:38:33.543319Z","iopub.status.idle":"2023-01-22T12:38:43.403958Z","shell.execute_reply.started":"2023-01-22T12:38:33.543266Z","shell.execute_reply":"2023-01-22T12:38:43.402587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"hands_list","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:39:02.547575Z","iopub.execute_input":"2023-01-22T12:39:02.549287Z","iopub.status.idle":"2023-01-22T12:39:02.564062Z","shell.execute_reply.started":"2023-01-22T12:39:02.549219Z","shell.execute_reply":"2023-01-22T12:39:02.562079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_hand_peaks","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:39:06.970835Z","iopub.execute_input":"2023-01-22T12:39:06.971307Z","iopub.status.idle":"2023-01-22T12:39:06.985988Z","shell.execute_reply.started":"2023-01-22T12:39:06.971270Z","shell.execute_reply":"2023-01-22T12:39:06.984436Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(len(vdos))\nlen(vdos[0]), vdos[0]","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:39:11.890843Z","iopub.execute_input":"2023-01-22T12:39:11.891632Z","iopub.status.idle":"2023-01-22T12:39:11.905933Z","shell.execute_reply.started":"2023-01-22T12:39:11.891525Z","shell.execute_reply":"2023-01-22T12:39:11.904172Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#one vdo generate hand keypoints\nvdo_canvas = generate_hand_keypoints_for_one_video(vdos[0])","metadata":{"execution":{"iopub.status.busy":"2023-03-11T04:50:36.619518Z","iopub.execute_input":"2023-03-11T04:50:36.620473Z","iopub.status.idle":"2023-03-11T04:58:33.555851Z","shell.execute_reply.started":"2023-03-11T04:50:36.620436Z","shell.execute_reply":"2023-03-11T04:58:33.554853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,20))\nplt.imshow(vdo_canvas[14]) #canvas[:, :, [2, 1, 0]]\nplt.axis('off')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-03-11T05:02:04.869096Z","iopub.execute_input":"2023-03-11T05:02:04.869460Z","iopub.status.idle":"2023-03-11T05:02:05.120588Z","shell.execute_reply.started":"2023-03-11T05:02:04.869426Z","shell.execute_reply":"2023-03-11T05:02:05.119586Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"type(vdos[0]),vdos[6]","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:50:00.004896Z","iopub.execute_input":"2023-01-22T12:50:00.005965Z","iopub.status.idle":"2023-01-22T12:50:00.016421Z","shell.execute_reply.started":"2023-01-22T12:50:00.005912Z","shell.execute_reply":"2023-01-22T12:50:00.014703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#for all vdo generate hand keypoints\nimport random\n\nnum_of_vdos = len(vdos)\nvdos_keypoints = []\nvdos_canvas = []\nfor i in range(num_of_vdos):\n    #vdo_canvas = generate_hand_keypoints_for_one_video(vdos[i])\n    vdo_canvas = []\n    vdo_keypoints = []\n    Num_of_frames = len(vdos[i])\n    print(i,Num_of_frames)\n    frame_index_list =  random.sample([i for i in range(Num_of_frames)],49)\n    for f in frame_index_list:\n        print(f)\n        try:\n            oriImg = vdos[0][f]\n        except:\n            continue\n        if(oriImg is None):\n            continue\n        candidate, subset = body_estimation(oriImg)\n        canvas = copy.deepcopy(oriImg)\n        hands_list = util.handDetect(candidate, subset, oriImg)\n        all_hand_peaks = get_all_hand_peaks(hands_list,oriImg)\n        canvas = util.draw_handpose(canvas, all_hand_peaks)\n        vdo_keypoints.append(all_hand_peaks)\n        vdo_canvas.append(canvas)\n    vdos_canvas.append(vdo_canvas)\n    vdos_keypoints.append(np.array(vdo_keypoints,dtype = 'float32'))","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:59:46.582092Z","iopub.execute_input":"2023-01-22T12:59:46.582552Z","iopub.status.idle":"2023-01-22T13:54:13.835417Z","shell.execute_reply.started":"2023-01-22T12:59:46.582512Z","shell.execute_reply":"2023-01-22T13:54:13.834405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ignore this box, I was experimenting\n#for one vdo, convert frames into np array\n'''vdo_frames = vdos[0][0]\nn = len(vdo_frames)\nvdo_frames_np_arr = []\nfor i in range(n):\n    vdo_frames_np_arr.append(np.array(vdo_frames[i],dtype = 'float32'))\n'''","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:50:00.019354Z","iopub.execute_input":"2023-01-22T12:50:00.020677Z","iopub.status.idle":"2023-01-22T12:50:00.030814Z","shell.execute_reply.started":"2023-01-22T12:50:00.020617Z","shell.execute_reply":"2023-01-22T12:50:00.029208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#useless ish, in our list of vdo each element is of type napari_video.napari_video.VideoReaderNP and the following code is converting it into np.ndarray\n#Each vdo has frames, each frame is of type np.ndarray\n\n#for all vdos\n'''\nvdo_np_arr = [] # list of np array of np array of frames\n#np array of frames is a vdo.\nfor j in range(len(vdos)):\n    vdo_frames = vdos[j]\n    n = len(vdo_frames)\n    vdo_frames_np_arr = []\n    for i in range(n):\n        vdo_frames_np_arr.append(np.array(vdo_frames[i],dtype = 'float32'))\n    vdo_np_arr.append(np.array(vdo_frames_np_arr,dtype = 'float32'))\n    '''","metadata":{"execution":{"iopub.status.busy":"2023-01-22T12:50:00.033921Z","iopub.execute_input":"2023-01-22T12:50:00.035336Z","iopub.status.idle":"2023-01-22T12:50:00.045959Z","shell.execute_reply.started":"2023-01-22T12:50:00.035278Z","shell.execute_reply":"2023-01-22T12:50:00.044129Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Ignore this box as well nash, again was trying to understand the object type and dimensions\n'''vr = vdos[0][0]\ntype(vdos[0][0]),type(vdos[0]), type(vdo_np_arr[0])'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(vdos_keypoints)","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:54:13.837335Z","iopub.execute_input":"2023-01-22T13:54:13.837717Z","iopub.status.idle":"2023-01-22T13:54:13.845568Z","shell.execute_reply.started":"2023-01-22T13:54:13.837681Z","shell.execute_reply":"2023-01-22T13:54:13.844583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#vdos_keypoints","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# f = open(\"VideoKeypoints.txt\",\"w+\")\n# for i in range(11):\n#     f.write(str(vdos_keypoints[i]))\n# f.close()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#shape=(1, 49, 2, 21, 2), 1 vdo, 49 frames, 2 hands, 21 keypoints of 2 dimensions. 21*2 = 42 for a hand. 2 hands so, 42*2 = 84\nX = pd.DataFrame(columns = [i for i in range(84*49)])\nX #each row will be a video of 30 frames. \n#pick 49 frames of vdo\ncol = 0\nfor vdo in vdos_keypoints:\n    for frame in vdo:\n        for hands in frame:\n            X[i],X[i+1] = map(list,zip(*hands))\n            i = i+2\nX","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:54:13.847168Z","iopub.execute_input":"2023-01-22T13:54:13.847726Z","iopub.status.idle":"2023-01-22T13:54:15.335092Z","shell.execute_reply.started":"2023-01-22T13:54:13.847690Z","shell.execute_reply":"2023-01-22T13:54:15.328523Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"obj_list = X.select_dtypes(include = 'object').columns.to_list()\nfor col in obj_list:\n    X[col] = le_y.fit_transform(X[col])","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:55:39.582066Z","iopub.execute_input":"2023-01-22T13:55:39.582411Z","iopub.status.idle":"2023-01-22T13:55:40.709140Z","shell.execute_reply.started":"2023-01-22T13:55:39.582381Z","shell.execute_reply":"2023-01-22T13:55:40.708187Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:55:44.642471Z","iopub.execute_input":"2023-01-22T13:55:44.642852Z","iopub.status.idle":"2023-01-22T13:55:44.963108Z","shell.execute_reply.started":"2023-01-22T13:55:44.642821Z","shell.execute_reply":"2023-01-22T13:55:44.962072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#1 vdo, 3 frames, 2 hands, 5 keypoints of 2D\n# temp = [[  [ [[0,0],[0,0],[1,1],[0,0],[2,2]] , [[0,0],[0,0],[1,1],[0,0],[2,2]] ]  ,  [ [[0,0],[0,0],[1,1],[0,0],[2,2]] , \n#     [[0,0],[0,0],[1,1],[0,0],[2,2]] ],  [ [[0,0],[0,0],[1,1],[0,0],[2,2]] , [[0,0],[0,0],[1,1],[0,0],[2,2]] ],   ]]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# X = pd.DataFrame(columns = [i for i in range(12)])\n# X #each row will be a video of 30 frames. \n# #pick 49 frames of vdo\n# i = 0\n# for vdo in temp:\n#     for frame in vdo:\n#         for hand in frame:\n#             X[i],X[i+1] = map(list,zip(*hand))\n#             i = i+2\n# X","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = X.tail(3)\nX_train = X.head(6)","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:00.890733Z","iopub.execute_input":"2023-01-22T13:59:00.891212Z","iopub.status.idle":"2023-01-22T13:59:00.911959Z","shell.execute_reply.started":"2023-01-22T13:59:00.891169Z","shell.execute_reply":"2023-01-22T13:59:00.910430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:12.348120Z","iopub.execute_input":"2023-01-22T13:59:12.348468Z","iopub.status.idle":"2023-01-22T13:59:12.355802Z","shell.execute_reply.started":"2023-01-22T13:59:12.348438Z","shell.execute_reply":"2023-01-22T13:59:12.354811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Caution runnning this takes alot of time, use X_test.shape instead\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:15.037325Z","iopub.execute_input":"2023-01-22T13:59:15.037694Z","iopub.status.idle":"2023-01-22T13:59:15.043926Z","shell.execute_reply.started":"2023-01-22T13:59:15.037664Z","shell.execute_reply":"2023-01-22T13:59:15.042960Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.info()","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:18.357227Z","iopub.execute_input":"2023-01-22T13:59:18.357591Z","iopub.status.idle":"2023-01-22T13:59:18.684857Z","shell.execute_reply.started":"2023-01-22T13:59:18.357559Z","shell.execute_reply":"2023-01-22T13:59:18.683775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_train","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:21.937323Z","iopub.execute_input":"2023-01-22T13:59:21.937685Z","iopub.status.idle":"2023-01-22T13:59:21.943939Z","shell.execute_reply.started":"2023-01-22T13:59:21.937655Z","shell.execute_reply":"2023-01-22T13:59:21.942904Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from xgboost import XGBClassifier\nmodel = XGBClassifier()","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:27.339330Z","iopub.execute_input":"2023-01-22T13:59:27.340076Z","iopub.status.idle":"2023-01-22T13:59:27.345802Z","shell.execute_reply.started":"2023-01-22T13:59:27.340031Z","shell.execute_reply":"2023-01-22T13:59:27.344652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.fit(X_train,Y_train)\nprint(model)","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:31.967355Z","iopub.execute_input":"2023-01-22T13:59:31.967724Z","iopub.status.idle":"2023-01-22T13:59:32.943051Z","shell.execute_reply.started":"2023-01-22T13:59:31.967688Z","shell.execute_reply":"2023-01-22T13:59:32.941611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred = model.predict(X_test)\nY_pred","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:36.837119Z","iopub.execute_input":"2023-01-22T13:59:36.837473Z","iopub.status.idle":"2023-01-22T13:59:36.960555Z","shell.execute_reply.started":"2023-01-22T13:59:36.837443Z","shell.execute_reply":"2023-01-22T13:59:36.959504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_pred_cat = le_y.inverse_transform(Y_pred)\nY_pred_cat","metadata":{"execution":{"iopub.status.busy":"2023-01-22T13:59:40.737393Z","iopub.execute_input":"2023-01-22T13:59:40.738045Z","iopub.status.idle":"2023-01-22T13:59:40.763337Z","shell.execute_reply.started":"2023-01-22T13:59:40.738011Z","shell.execute_reply":"2023-01-22T13:59:40.761364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Use images along with the keypoints vectors to classify","metadata":{}}]}